import json
import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import jieba.analyse
import networkx as nx
from collections import Counter
from wordcloud import WordCloud
from snownlp import SnowNLP
import requests
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from PIL import Image
import matplotlib
import argparse
from utils import get_data_loader, clean_text

matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['axes.unicode_minus'] = False

# DeepSeek API é…ç½®
DEEPSEEK_API_KEY = "sk-8dc4ee20fc8a40a4906dc872534a53a7"
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

# è·å–å‘½ä»¤è¡Œå‚æ•°
def get_platform_from_args():
    parser = argparse.ArgumentParser(description='ç¤¾äº¤åª’ä½“æ•°æ®åˆ†æå·¥å…·')
    parser.add_argument('--platform', type=str, default='xhs', 
                        choices=['xhs', 'dy'], 
                        help='æŒ‡å®šè¦åˆ†æçš„å¹³å°: xhs (å°çº¢ä¹¦) æˆ– dy (æŠ–éŸ³)')
    args, _ = parser.parse_known_args()
    return args.platform

# è¯é¢‘åˆ†æ
def word_frequency_analysis(text_series):
    # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
    all_text = ' '.join(text_series.astype(str).tolist())
    
    # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
    jieba.setLogLevel(20)  # è®¾ç½®jiebaçš„æ—¥å¿—çº§åˆ«ï¼Œé¿å…è¾“å‡ºè¿‡å¤šä¿¡æ¯
    words = jieba.cut(all_text)
    
    # è¿‡æ»¤åœç”¨è¯
    stopwords = ['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™']
    filtered_words = [word for word in words if len(word) > 1 and word not in stopwords]
    
    # ç»Ÿè®¡è¯é¢‘
    word_counts = Counter(filtered_words)
    
    # è·å–å‰30ä¸ªé«˜é¢‘è¯
    top_words = word_counts.most_common(30)
    
    return top_words, all_text

# ç”Ÿæˆè¯äº‘
def generate_wordcloud(text):
    wordcloud = WordCloud(font_path='simhei.ttf', width=800, height=400, 
                         background_color='white', max_words=100).generate(text)
    
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.tight_layout()
    
    # ä¿å­˜è¯äº‘å›¾
    wordcloud_path = 'wordcloud.png'
    plt.savefig(wordcloud_path)
    plt.close()
    
    return wordcloud_path

# è¯­ä¹‰ç½‘ç»œåˆ†æ
def semantic_network_analysis(top_words, text):
    # åˆ›å»ºå›¾
    G = nx.Graph()
    
    # æ·»åŠ èŠ‚ç‚¹
    for word, count in top_words:
        G.add_node(word, weight=count)
    
    # æ·»åŠ è¾¹ - åŸºäºå…±ç°å…³ç³»
    words_dict = dict(top_words)
    words_list = list(words_dict.keys())
    
    # æ£€æŸ¥æ¯å¯¹é«˜é¢‘è¯æ˜¯å¦åœ¨åŒä¸€æ®µæ–‡æœ¬ä¸­å…±ç°
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
    for i, word1 in enumerate(words_list):
        for word2 in words_list[i+1:]:
            weight = 0
            for sentence in sentences:
                if word1 in sentence and word2 in sentence:
                    weight += 1
            if weight > 0:
                G.add_edge(word1, word2, weight=weight)
    
    # ç»˜åˆ¶è¯­ä¹‰ç½‘ç»œå›¾
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G, seed=42)  # ä½¿ç”¨springå¸ƒå±€
    
    # èŠ‚ç‚¹å¤§å°åŸºäºè¯é¢‘
    node_size = [G.nodes[node]['weight'] * 20 for node in G.nodes()]
    
    # è¾¹çš„å®½åº¦åŸºäºå…±ç°é¢‘ç‡
    edge_width = [G[u][v]['weight'] / 2 for u, v in G.edges()]
    
    nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='skyblue', alpha=0.8)
    nx.draw_networkx_edges(G, pos, width=edge_width, alpha=0.5, edge_color='gray')
    nx.draw_networkx_labels(G, pos, font_size=10, font_family='SimHei')
    
    plt.axis('off')
    plt.tight_layout()
    
    # ä¿å­˜è¯­ä¹‰ç½‘ç»œå›¾
    network_path = 'semantic_network.png'
    plt.savefig(network_path)
    plt.close()
    
    return network_path

# æƒ…æ„Ÿå€¾å‘åˆ†æ
def sentiment_analysis(text_series):
    sentiments = []
    for text in text_series:
        if pd.isna(text) or text == '':
            sentiments.append(0.5)  # ä¸­æ€§
            continue
        try:
            s = SnowNLP(str(text))
            sentiments.append(s.sentiments)
        except:
            sentiments.append(0.5)  # å¤„ç†å¼‚å¸¸æƒ…å†µ
    
    # åˆ›å»ºæƒ…æ„Ÿåˆ†å¸ƒå›¾
    plt.figure(figsize=(10, 6))
    sns.histplot(sentiments, bins=20, kde=True)
    plt.title('æƒ…æ„Ÿå€¾å‘åˆ†å¸ƒ')
    plt.xlabel('æƒ…æ„Ÿå¾—åˆ† (0:æ¶ˆæ - 1:ç§¯æ)')
    plt.ylabel('é¢‘æ¬¡')
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # ä¿å­˜æƒ…æ„Ÿåˆ†æå›¾
    sentiment_path = 'sentiment_analysis.png'
    plt.savefig(sentiment_path)
    plt.close()
    
    # è®¡ç®—æƒ…æ„Ÿç»Ÿè®¡æ•°æ®
    positive = sum(1 for s in sentiments if s > 0.7)
    neutral = sum(1 for s in sentiments if 0.3 <= s <= 0.7)
    negative = sum(1 for s in sentiments if s < 0.3)
    total = len(sentiments)
    
    sentiment_stats = {
        'positive': positive,
        'positive_percent': positive / total * 100 if total > 0 else 0,
        'neutral': neutral,
        'neutral_percent': neutral / total * 100 if total > 0 else 0,
        'negative': negative,
        'negative_percent': negative / total * 100 if total > 0 else 0,
    }
    
    return sentiment_path, sentiment_stats, sentiments

# è°ƒç”¨DeepSeek APIè¿›è¡Œä¸‰çº§ç¼–ç 
def deepseek_analysis(text):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
    }
    
    # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¶…å‡ºAPIé™åˆ¶
    if len(text) > 8000:
        text = text[:8000]
    
    prompt = f"""
    è¯·å¯¹ä»¥ä¸‹ç½‘ç»œæ–‡æœ¬æ•°æ®è¿›è¡Œä¸‰çº§ç¼–ç åˆ†æï¼Œæå–å…³é”®ä¸»é¢˜å’Œæ¨¡å¼ï¼š
    
    {text}
    
    è¯·æŒ‰ç…§ä»¥ä¸‹ä¸‰çº§ç¼–ç ç»“æ„è¿›è¡Œåˆ†æï¼š
    1. å¼€æ”¾å¼ç¼–ç ï¼šè¯†åˆ«åŸå§‹æ•°æ®ä¸­çš„å…³é”®æ¦‚å¿µå’Œç±»åˆ«
    2. ä¸»è½´ç¼–ç ï¼šç¡®å®šè¿™äº›æ¦‚å¿µä¹‹é—´çš„å…³ç³»å’Œè”ç³»
    3. é€‰æ‹©æ€§ç¼–ç ï¼šæ•´åˆæ‰€æœ‰ç±»åˆ«ï¼Œç¡®å®šæ ¸å¿ƒä¸»é¢˜
    
    å¯¹äºæ¯ä¸ªçº§åˆ«ï¼Œè¯·æä¾›è¯¦ç»†çš„ç¼–ç ç»“æœå’Œåˆ†æã€‚
    """
    
    data = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 2000
    }
    
    try:
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data)
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content']
    except Exception as e:
        return f"APIè°ƒç”¨å¤±è´¥: {str(e)}"

# Streamlitåº”ç”¨
def main():
    # è·å–å¹³å°å‚æ•°
    platform = get_platform_from_args()
    
    # è®¾ç½®é¡µé¢é…ç½®
    platform_name = "å°çº¢ä¹¦" if platform == "xhs" else "æŠ–éŸ³"
    st.set_page_config(page_title=f"{platform_name}æ•°æ®åˆ†æ", page_icon="ğŸ“Š", layout="wide")
    
    st.title(f"{platform_name}ç½‘ç»œæ–‡æœ¬æ•°æ®åˆ†æ")
    
    # å¹³å°é€‰æ‹©å™¨
    platform_options = {"xhs": "å°çº¢ä¹¦", "dy": "æŠ–éŸ³"}
    selected_platform = st.sidebar.selectbox(
        "é€‰æ‹©åˆ†æå¹³å°",
        options=list(platform_options.keys()),
        format_func=lambda x: platform_options[x],
        index=list(platform_options.keys()).index(platform)
    )
    
    # è·å–å¯¹åº”å¹³å°çš„æ•°æ®åŠ è½½å™¨
    data_loader = get_data_loader(selected_platform)
    
    # åŠ è½½æ•°æ®
    with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®..."):
        comments_data, contents_data = data_loader.load_data()
        # æ•°æ®æ¸…æ´—
        comments_data, contents_data = data_loader.clean_data(comments_data, contents_data)
    
    # ç»Ÿè®¡æœ‰æ•ˆæ•°æ®æ•°é‡
    valid_comments = [comment for comment in comments_data if comment.get('content')]
    valid_contents = [content for content in contents_data if content.get('desc') or content.get('title')]
    valid_comments_count = len(valid_comments)
    valid_contents_count = len(valid_contents)
    
    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
    st.header("1. æ•°æ®ç»Ÿè®¡")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("è¯„è®ºæ•°æ®æ•°é‡", valid_comments_count)
    with col2:
        st.metric("å†…å®¹æ•°æ®æ•°é‡", valid_contents_count)
    
    # æå–æœ‰æ•ˆå†…å®¹
    comments_df = data_loader.extract_comments(comments_data)
    contents_df = data_loader.extract_contents(contents_data)
    
    # æ•°æ®é¢„è§ˆ
    st.header("2. æ•°æ®é¢„è§ˆ")
    tab1, tab2 = st.tabs(["è¯„è®ºæ•°æ®", "å†…å®¹æ•°æ®"])
    
    with tab1:
        st.subheader("è¯„è®ºæ•°æ®")
        st.dataframe(comments_df.head(10))
        
        # è¯„è®ºæ•°æ®ä¸‹è½½
        csv = comments_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ä¸‹è½½è¯„è®ºæ•°æ®CSV",
            data=csv,
            file_name="comments_data.csv",
            mime="text/csv"
        )
    
    with tab2:
        st.subheader("å†…å®¹æ•°æ®")
        st.dataframe(contents_df.head(10))
        
        # å†…å®¹æ•°æ®ä¸‹è½½
        csv = contents_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ä¸‹è½½å†…å®¹æ•°æ®CSV",
            data=csv,
            file_name="contents_data.csv",
            mime="text/csv"
        )
    
    # èˆ†æƒ…åˆ†æ
    st.header("3. èˆ†æƒ…åˆ†æ")
    
    # è¯é¢‘åˆ†æ
    st.subheader("3.1 è¯é¢‘åˆ†æ")
    top_words, all_text = word_frequency_analysis(comments_df['content'])
    
    # æ˜¾ç¤ºé«˜é¢‘è¯
    df_top_words = pd.DataFrame(top_words, columns=['è¯è¯­', 'é¢‘æ¬¡'])
    col1, col2 = st.columns([2, 3])
    
    with col1:
        st.dataframe(df_top_words)
    
    with col2:
        fig = px.bar(df_top_words, x='è¯è¯­', y='é¢‘æ¬¡', title='é«˜é¢‘è¯ç»Ÿè®¡')
        st.plotly_chart(fig, use_container_width=True)
    
    # è¯äº‘ç”Ÿæˆ
    st.subheader("3.2 è¯äº‘åˆ†æ")
    wordcloud_path = generate_wordcloud(all_text)
    st.image(wordcloud_path, caption='è¯„è®ºè¯äº‘')
    
    # è¯­ä¹‰ç½‘ç»œåˆ†æ
    st.subheader("3.3 è¯­ä¹‰ç½‘ç»œåˆ†æ")
    network_path = semantic_network_analysis(top_words, all_text)
    st.image(network_path, caption='è¯­ä¹‰ç½‘ç»œå›¾')
    
    # æƒ…æ„Ÿåˆ†æ
    st.subheader("3.4 æƒ…æ„Ÿå€¾å‘åˆ†æ")
    sentiment_path, sentiment_stats, sentiments = sentiment_analysis(comments_df['content'])
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.image(sentiment_path, caption='æƒ…æ„Ÿåˆ†å¸ƒå›¾')
    
    with col2:
        # åˆ›å»ºé¥¼å›¾
        labels = ['ç§¯æ', 'ä¸­æ€§', 'æ¶ˆæ']
        values = [sentiment_stats['positive_percent'], 
                 sentiment_stats['neutral_percent'], 
                 sentiment_stats['negative_percent']]
        
        fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])
        fig.update_layout(title_text='æƒ…æ„Ÿå€¾å‘æ¯”ä¾‹')
        st.plotly_chart(fig, use_container_width=True)
    
    # DeepSeek APIåˆ†æ
    st.header("4. DeepSeekæ–‡æœ¬åˆ†æä¸ä¸‰çº§ç¼–ç ")
    
    # åˆå¹¶è¯„è®ºå’Œå†…å®¹æ–‡æœ¬ç”¨äºåˆ†æ
    analysis_text = "\n\n".join(comments_df['content'].dropna().astype(str).tolist()[:50])
    analysis_text += "\n\n" + "\n\n".join(contents_df['desc'].dropna().astype(str).tolist()[:20])
    
    # æ–‡æœ¬ç¼–è¾‘åŒº
    edited_text = st.text_area("ç¼–è¾‘ç”¨äºåˆ†æçš„æ–‡æœ¬", analysis_text, height=200)
    
    if st.button("è¿›è¡ŒDeepSeekåˆ†æ"):
        with st.spinner("æ­£åœ¨è°ƒç”¨DeepSeek APIè¿›è¡Œåˆ†æ..."):
            analysis_result = deepseek_analysis(edited_text)
            st.markdown("### ä¸‰çº§ç¼–ç åˆ†æç»“æœ")
            st.markdown(analysis_result)
            
            # æä¾›å¤åˆ¶åŠŸèƒ½
            st.text_area("å¤åˆ¶åˆ†æç»“æœ", analysis_result, height=300)
    
    # ç»“æœå¯¼å‡º
    st.header("5. ç»“æœå¯¼å‡º")
    st.markdown("""
    æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯¼å‡ºåˆ†æç»“æœï¼š
    1. ä½¿ç”¨ä¸Šæ–¹çš„ä¸‹è½½æŒ‰é’®ä¸‹è½½åŸå§‹æ•°æ®
    2. å³é”®ç‚¹å‡»å›¾è¡¨ä¿å­˜å›¾åƒ
    3. å¤åˆ¶æ–‡æœ¬åˆ†æç»“æœç²˜è´´åˆ°æ‚¨çš„è®ºæ–‡ä¸­
    """)

if __name__ == "__main__":
    main()